{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7cb41d5f",
      "metadata": {
        "id": "7cb41d5f"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05e301b9",
      "metadata": {
        "id": "05e301b9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c0d2b7",
      "metadata": {
        "id": "b9c0d2b7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "701614cb",
      "metadata": {
        "id": "701614cb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "865ce599",
      "metadata": {
        "id": "865ce599"
      },
      "source": [
        "## MLflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebf9f395",
      "metadata": {
        "id": "ebf9f395"
      },
      "source": [
        "##### What is MLflow primarily used for?\n",
        "\n",
        "    - a) Data preprocessing\n",
        "    - b) Model training\n",
        "    - c) Machine learning lifecycle management\n",
        "    - d) Web development"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "682da4f7",
      "metadata": {
        "id": "682da4f7"
      },
      "source": [
        "##### Which component of MLflow is used to track experiments?\n",
        "    - a) MLflow Tracking\n",
        "    - b) MLflow Projects\n",
        "    - c) MLflow Models\n",
        "    - d) MLflow Registry\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ac846e6",
      "metadata": {
        "id": "2ac846e6"
      },
      "source": [
        "###### How can MLflow be integrated with popular ML frameworks?\n",
        "    a) Through APIs and autologging\n",
        "    b) Using SQL queries\n",
        "    c) Only through manual logging\n",
        "    d) By writing custom scripts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7021bd3b",
      "metadata": {
        "id": "7021bd3b"
      },
      "source": [
        "###### MLflow allows logging which of the following?\n",
        "    a) Metrics\n",
        "    b) Parameters\n",
        "    c) Artifacts\n",
        "    d) All of the above"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22b187d5",
      "metadata": {
        "id": "22b187d5"
      },
      "source": [
        "###### Which command is used to start the MLflow UI?\n",
        "    a) mlflow start-ui\n",
        "    b) mlflow ui\n",
        "    c) mlflow launch-ui\n",
        "    d) mlflow dashboard"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff9516eb",
      "metadata": {
        "id": "ff9516eb"
      },
      "source": [
        "###### What file format does MLflow use to store model metadata?\n",
        "    a) JSON\n",
        "    b) YAML\n",
        "    c) CSV\n",
        "    d) XML"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac5d05b9",
      "metadata": {
        "id": "ac5d05b9"
      },
      "source": [
        "## TensorBoard"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d71b63f",
      "metadata": {
        "id": "2d71b63f"
      },
      "source": [
        "###### TensorBoard is mainly used for:\n",
        "    a) Model deployment\n",
        "    b) Experiment visualization\n",
        "    c) Data collection\n",
        "    d) Image processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "319c23c4",
      "metadata": {
        "id": "319c23c4"
      },
      "source": [
        "###### What types of data can TensorBoard visualize?\n",
        "    a) Scalars\n",
        "    b) Images\n",
        "    c) Histograms\n",
        "    d) All of the above"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bcf25d1",
      "metadata": {
        "id": "0bcf25d1"
      },
      "source": [
        "###### Which command is used to launch TensorBoard?\n",
        "    a) tensorboard --logdir=<path>\n",
        "    b) tensorboard start\n",
        "    c) tensorboard run\n",
        "    d) tensorboard launch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b112205",
      "metadata": {
        "id": "8b112205"
      },
      "source": [
        "###### TensorBoard logs data in which directory format?\n",
        "    a) .tensorboard\n",
        "    b) ./logs/\n",
        "    c) .logdir\n",
        "    d) ./output/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ba4e65f",
      "metadata": {
        "id": "5ba4e65f"
      },
      "source": [
        "###### In TensorBoard, how are logs typically stored?\n",
        "    a) SQL Database\n",
        "    b) Event files\n",
        "    c) JSON files\n",
        "    d) CSV files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "076ed854",
      "metadata": {
        "id": "076ed854"
      },
      "source": [
        "##### Which ML framework primarily integrates with TensorBoard?\n",
        "    a) PyTorch\n",
        "    b) TensorFlow\n",
        "    c) Scikit-learn\n",
        "    d) XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b98001b4",
      "metadata": {
        "id": "b98001b4"
      },
      "source": [
        "## TensorBoard"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5aaba23",
      "metadata": {
        "id": "d5aaba23"
      },
      "source": [
        "###### What is cloud computing?\n",
        "    a) A programming language\n",
        "    b) A storage device\n",
        "    c) On-demand delivery of computing services over the internet\n",
        "    d) A networking protocol"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9e89eb2",
      "metadata": {
        "id": "a9e89eb2"
      },
      "source": [
        "###### Which of the following is not a cloud computing model?\n",
        "    a) IaaS\n",
        "    b) PaaS\n",
        "    c) SaaS\n",
        "    d) HTTP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d5a3b37",
      "metadata": {
        "id": "3d5a3b37"
      },
      "source": [
        "###### AWS, Google Cloud, and Azure provide which type of cloud service?\n",
        "    a) Private cloud\n",
        "    b) Public cloud\n",
        "    c) Hybrid cloud\n",
        "    d) On-premises cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfa02e6a",
      "metadata": {
        "id": "bfa02e6a"
      },
      "source": [
        "##### Which cloud computing model provides the highest level of user control?\n",
        "    a) SaaS\n",
        "    b) PaaS\n",
        "    c) IaaS\n",
        "    d) FaaS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9205dd2b",
      "metadata": {
        "id": "9205dd2b"
      },
      "source": [
        "##### Which cloud service provider offers Lambda for serverless computing?\n",
        "    a) Google Cloud\n",
        "    b) AWS\n",
        "    c) Azure\n",
        "    d) IBM Cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d047a86",
      "metadata": {
        "id": "5d047a86"
      },
      "source": [
        "###### What does \"elasticity\" in cloud computing refer to?\n",
        "    a) Automatic scaling of resources\n",
        "    b) Data compression\n",
        "    c) Network security\n",
        "    d) Storage encryption"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d6fc5cc",
      "metadata": {
        "id": "6d6fc5cc"
      },
      "source": [
        "## Flask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca62acc7",
      "metadata": {
        "id": "ca62acc7"
      },
      "source": [
        "##### Flask is a framework for which programming language?\n",
        "    a) Java\n",
        "    b) Python\n",
        "    c) C++\n",
        "    d) JavaScript"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b0c3fce",
      "metadata": {
        "id": "3b0c3fce"
      },
      "source": [
        "##### Which function is used to define a route in Flask?\n",
        "    a) flask.route()\n",
        "    b) app.route()\n",
        "    c) route.app()\n",
        "    d) flask.define_route()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e166949c",
      "metadata": {
        "id": "e166949c"
      },
      "source": [
        "##### What is the default port for Flask applications?\n",
        "    a) 5000\n",
        "    b) 8000\n",
        "    c) 8080\n",
        "    d) 3000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ce1187d",
      "metadata": {
        "id": "7ce1187d"
      },
      "source": [
        "##### How do you run a Flask application?\n",
        "    a) python app.py\n",
        "    b) flask run\n",
        "    c) python -m flask run\n",
        "    d) All of the above\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f12b3af6",
      "metadata": {
        "id": "f12b3af6"
      },
      "source": [
        "##### What is the purpose of Flask-SQLAlchemy?\n",
        "    a) Web scraping\n",
        "    b) ORM for Flask applications\n",
        "    c) Flask UI development\n",
        "    d) API testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "813b0c55",
      "metadata": {
        "id": "813b0c55"
      },
      "source": [
        "##### How do you return a JSON response in Flask?\n",
        "    a) return jsonify(data)\n",
        "    b) return json(data)\n",
        "    c) return as_json(data)\n",
        "    d) return to_json(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f29d9c34",
      "metadata": {
        "id": "f29d9c34"
      },
      "source": [
        "## Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d225edbd",
      "metadata": {
        "id": "d225edbd"
      },
      "source": [
        "###### In reinforcement learning, what is an \"agent\"?\n",
        "    a) A dataset\n",
        "    b) A learning algorithm\n",
        "    c) An entity that interacts with the environment\n",
        "    d) A loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f82b162",
      "metadata": {
        "id": "8f82b162"
      },
      "source": [
        "##### What does \"reward\" in reinforcement learning represent?\n",
        "    a) A measure of performance\n",
        "    b) An optimization function\n",
        "    c) A dataset\n",
        "    d) A probability distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f71abb84",
      "metadata": {
        "id": "f71abb84"
      },
      "source": [
        "##### Which algorithm is commonly used in reinforcement learning?\n",
        "    a) Q-learning\n",
        "    b) Linear regression\n",
        "    c) K-means clustering\n",
        "    d) Na√Øve Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ec6fa49",
      "metadata": {
        "id": "9ec6fa49"
      },
      "source": [
        "##### What is an \"episode\" in reinforcement learning?\n",
        "    a) A complete sequence of states, actions, and rewards\n",
        "    b) A single step in learning\n",
        "    c) A specific loss function\n",
        "    d) A random decision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df941db4",
      "metadata": {
        "id": "df941db4"
      },
      "source": [
        "##### What is the exploration-exploitation tradeoff in RL?\n",
        "    a) Balancing between learning new things and maximizing rewards\n",
        "    b) Choosing between two actions randomly\n",
        "    c) Running two different algorithms\n",
        "    d) Ignoring rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab89a1f4",
      "metadata": {
        "id": "ab89a1f4"
      },
      "source": [
        "##### What is \"policy\" in reinforcement learning?\n",
        "    a) A function mapping states to actions\n",
        "    b) A loss function\n",
        "    c) A reward function\n",
        "    d) A dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fff823af",
      "metadata": {
        "id": "fff823af"
      },
      "source": [
        "## Unit Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6ce4ab2",
      "metadata": {
        "id": "e6ce4ab2"
      },
      "source": [
        "##### Which library is commonly used for unit testing in Python?\n",
        "    a) pytest\n",
        "    b) unittest\n",
        "    c) Both a and b\n",
        "    d) None of the above"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb4b5581",
      "metadata": {
        "id": "cb4b5581"
      },
      "source": [
        "##### What is the main purpose of unit testing?\n",
        "    a) Test entire application at once\n",
        "    b) Validate individual components of a program\n",
        "    c) Check database connectivity\n",
        "    d) Monitor real-time logs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db48f2dc",
      "metadata": {
        "id": "db48f2dc"
      },
      "source": [
        "## Pylint"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f25f66bc",
      "metadata": {
        "id": "f25f66bc"
      },
      "source": [
        "##### What is the purpose of Pylint?\n",
        "    a) Debugging\n",
        "    b) Checking code quality\n",
        "    c) Writing documentation\n",
        "    d) Performance testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "929b7bc6",
      "metadata": {
        "id": "929b7bc6"
      },
      "source": [
        "## Apache Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9afa080b",
      "metadata": {
        "id": "9afa080b"
      },
      "source": [
        "##### Apache Spark is primarily used for:\n",
        "    a) Web development\n",
        "    b) Distributed data processing\n",
        "    c) Image processing\n",
        "    d) Mobile application development\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85b032b4",
      "metadata": {
        "id": "85b032b4"
      },
      "source": [
        "## Unit Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "544719c4",
      "metadata": {
        "id": "544719c4"
      },
      "source": [
        "###### Which of the following is a benefit of unit testing?\n",
        "    a) Reduces debugging time\n",
        "    b) Improves code quality\n",
        "    c) Helps catch errors early\n",
        "    d) All of the above"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "859278f3",
      "metadata": {
        "id": "859278f3"
      },
      "source": [
        "###### In unittest, which method is used to check if two values are equal?\n",
        "    a) assertEqual()\n",
        "    b) assertTrue()\n",
        "    c) assertIs()\n",
        "    d) assertNotEqual()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55e6afc1",
      "metadata": {
        "id": "55e6afc1"
      },
      "source": [
        "##### What does pytest use to automatically discover test files?\n",
        "    a) Files starting with test_\n",
        "    b) Files ending with _test.py\n",
        "    c) Both a and b\n",
        "    d) None of the above"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05bd815b",
      "metadata": {
        "id": "05bd815b"
      },
      "source": [
        "###### How do you run all test cases in pytest?\n",
        "    a) pytest run all\n",
        "    b) pytest\n",
        "    c) pytest test.py\n",
        "    d) python -m pytest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cacc22a9",
      "metadata": {
        "id": "cacc22a9"
      },
      "source": [
        "## Pylint"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcfd6e59",
      "metadata": {
        "id": "fcfd6e59"
      },
      "source": [
        "##### What is Pylint used for in Python?\n",
        "    a) Detecting syntax errors\n",
        "    b) Enforcing coding standards\n",
        "    c) Both a and b\n",
        "    d) Running performance tests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6410f401",
      "metadata": {
        "id": "6410f401"
      },
      "source": [
        "###### Which command runs Pylint on a Python file?\n",
        "    a) pylint filename.py\n",
        "    b) python -m pylint filename.py\n",
        "    c) Both a and b\n",
        "    d) pylint run filename.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ec85c9a",
      "metadata": {
        "id": "1ec85c9a"
      },
      "source": [
        "##### Pylint scores code quality on a scale of:\n",
        "    a) 0 to 5\n",
        "    b) 0 to 10\n",
        "    c) -10 to 10\n",
        "    d) 0 to 100"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a969fd3b",
      "metadata": {
        "id": "a969fd3b"
      },
      "source": [
        "###### Which of the following is NOT checked by Pylint?\n",
        "    a) Code formatting\n",
        "    b) Logic errors\n",
        "    c) Variable naming conventions\n",
        "    d) Run-time performance\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c455fb2",
      "metadata": {
        "id": "0c455fb2"
      },
      "source": [
        "##### Pylint can be configured using which file format?\n",
        "    a) JSON\n",
        "    b) .pylintrc\n",
        "    c) YAML\n",
        "    d) XML"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10d9fa43",
      "metadata": {
        "id": "10d9fa43"
      },
      "source": [
        "## Apache Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f68d9327",
      "metadata": {
        "id": "f68d9327"
      },
      "source": [
        "##### Apache Spark is written in which programming language?\n",
        "    a) Python\n",
        "    b) Java\n",
        "    c) Scala\n",
        "    d) C++"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adf25718",
      "metadata": {
        "id": "adf25718"
      },
      "source": [
        "##### Which component of Apache Spark is used for real-time data processing?\n",
        "    a) Spark SQL\n",
        "    b) Spark Streaming\n",
        "    c) MLlib\n",
        "    d) GraphX\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e85cef9b",
      "metadata": {
        "id": "e85cef9b"
      },
      "source": [
        "###### What is the default cluster manager for Apache Spark?\n",
        "    a) Kubernetes\n",
        "    b) Hadoop YARN\n",
        "    c) Spark Standalone\n",
        "    d) Apache Mesos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41af954e",
      "metadata": {
        "id": "41af954e"
      },
      "source": [
        "###### Which API in Apache Spark is used for batch processing?\n",
        "    a) RDD\n",
        "    b) Spark SQL\n",
        "    c) Spark Streaming\n",
        "    d) MLlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "245a7ee1",
      "metadata": {
        "id": "245a7ee1"
      },
      "source": [
        "##### What is the primary abstraction in Spark?\n",
        "    a) DataFrame\n",
        "    b) Dataset\n",
        "    c) RDD\n",
        "    d) Table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10e2b4af",
      "metadata": {
        "id": "10e2b4af"
      },
      "source": [
        "###### What is a DataFrame in Spark?\n",
        "    a) A distributed collection of data organized into named columns\n",
        "    b) A type of database\n",
        "    c) A visualization tool\n",
        "    d) A low-level API for cluster computing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLflow**\n",
        "\n",
        "1. **What is MLflow primarily used for?**\n",
        "   - **c) Machine learning lifecycle management**\n",
        "\n",
        "2. **Which component of MLflow is used to track experiments?**\n",
        "   - **a) MLflow Tracking**\n",
        "\n",
        "3. **How can MLflow be integrated with popular ML frameworks?**\n",
        "   - **a) Through APIs and autologging**\n",
        "\n",
        "4. **MLflow allows logging which of the following?**\n",
        "   - **d) All of the above** (Metrics, Parameters, Artifacts)\n",
        "\n",
        "5. **Which command is used to start the MLflow UI?**\n",
        "   - **b) mlflow ui**\n",
        "\n",
        "6. **What file format does MLflow use to store model metadata?**\n",
        "   - **a) JSON**\n",
        "\n",
        "**TensorBoard**\n",
        "\n",
        "1. **TensorBoard is mainly used for:**\n",
        "   - **b) Experiment visualization**\n",
        "\n",
        "2. **What types of data can TensorBoard visualize?**\n",
        "   - **d) All of the above** (Scalars, Images, Histograms)\n",
        "\n",
        "3. **Which command is used to launch TensorBoard?**\n",
        "   - **a) tensorboard --logdir=<path>**\n",
        "\n",
        "4. **TensorBoard logs data in which directory format?**\n",
        "   - **b) ./logs/**\n",
        "\n",
        "5. **In TensorBoard, how are logs typically stored?**\n",
        "   - **b) Event files**\n",
        "\n",
        "6. **Which ML framework primarily integrates with TensorBoard?**\n",
        "   - **b) TensorFlow**\n",
        "\n",
        "**Cloud Computing**\n",
        "\n",
        "1. **What is cloud computing?**\n",
        "   - **c) On-demand delivery of computing services over the internet**\n",
        "\n",
        "2. **Which of the following is not a cloud computing model?**\n",
        "   - **d) HTTP**\n",
        "\n",
        "3. **AWS, Google Cloud, and Azure provide which type of cloud service?**\n",
        "   - **b) Public cloud**\n",
        "\n",
        "4. **Which cloud computing model provides the highest level of user control?**\n",
        "   - **c) IaaS**\n",
        "\n",
        "5. **Which cloud service provider offers Lambda for serverless computing?**\n",
        "   - **b) AWS**\n",
        "\n",
        "6. **What does \"elasticity\" in cloud computing refer to?**\n",
        "   - **a) Automatic scaling of resources**\n",
        "\n",
        "**Flask**\n",
        "\n",
        "1. **Flask is a framework for which programming language?**\n",
        "   - **b) Python**\n",
        "\n",
        "2. **Which function is used to define a route in Flask?**\n",
        "   - **b) app.route()**\n",
        "\n",
        "3. **What is the default port for Flask applications?**\n",
        "   - **a) 5000**\n",
        "\n",
        "4. **How do you run a Flask application?**\n",
        "   - **d) All of the above** (python app.py, flask run, python -m flask run)\n",
        "\n",
        "5. **What is the purpose of Flask-SQLAlchemy?**\n",
        "   - **b) ORM for Flask applications**\n",
        "\n",
        "6. **How do you return a JSON response in Flask?**\n",
        "   - **a) return jsonify(data)**\n",
        "\n",
        "**Reinforcement Learning**\n",
        "\n",
        "1. **In reinforcement learning, what is an \"agent\"?**\n",
        "   - **c) An entity that interacts with the environment**\n",
        "\n",
        "2. **What does \"reward\" in reinforcement learning represent?**\n",
        "   - **a) A measure of performance**\n",
        "\n",
        "3. **Which algorithm is commonly used in reinforcement learning?**\n",
        "   - **a) Q-learning**\n",
        "\n",
        "4. **What is an \"episode\" in reinforcement learning?**\n",
        "   - **a) A complete sequence of states, actions, and rewards**\n",
        "\n",
        "5. **What is the exploration-exploitation tradeoff in RL?**\n",
        "   - **a) Balancing between learning new things and maximizing rewards**\n",
        "\n",
        "6. **What is \"policy\" in reinforcement learning?**\n",
        "   - **a) A function mapping states to actions**\n",
        "\n",
        "**Unit Testing**\n",
        "\n",
        "1. **Which library is commonly used for unit testing in Python?**\n",
        "   - **c) Both a and b** (pytest, unittest)\n",
        "\n",
        "2. **What is the main purpose of unit testing?**\n",
        "   - **b) Validate individual components of a program**\n",
        "\n",
        "3. **Which of the following is a benefit of unit testing?**\n",
        "   - **d) All of the above** (Reduces debugging time, Improves code quality, Helps catch errors early)\n",
        "\n",
        "4. **In unittest, which method is used to check if two values are equal?**\n",
        "   - **a) assertEqual()**\n",
        "\n",
        "5. **What does pytest use to automatically discover test files?**\n",
        "   - **c) Both a and b** (Files starting with test_, Files ending with _test.py)\n",
        "\n",
        "6. **How do you run all test cases in pytest?**\n",
        "   - **b) pytest**\n",
        "\n",
        "**Pylint**\n",
        "\n",
        "1. **What is the purpose of Pylint?**\n",
        "   - **c) Both a and b** (Detecting syntax errors, Enforcing coding standards)\n",
        "\n",
        "2. **Which command runs Pylint on a Python file?**\n",
        "   - **c) Both a and b** (pylint filename.py, python -m pylint filename.py)\n",
        "\n",
        "3. **Pylint scores code quality on a scale of:**\n",
        "   - **b) 0 to 10**\n",
        "\n",
        "4. **Which of the following is NOT checked by Pylint?**\n",
        "   - **d) Run-time performance**\n",
        "\n",
        "5. **Pylint can be configured using which file format?**\n",
        "   - **b) .pylintrc**\n",
        "\n",
        "**Apache Spark**\n",
        "\n",
        "1. **Apache Spark is written in which programming language?**\n",
        "   - **c) Scala**\n",
        "\n",
        "2. **Which component of Apache Spark is used for real-time data processing?**\n",
        "   - **b) Spark Streaming**\n",
        "\n",
        "3. **What is the default cluster manager for Apache Spark?**\n",
        "   - **c) Spark Standalone**\n",
        "\n",
        "4. **Which API in Apache Spark is used for batch processing?**\n",
        "   - **a) RDD**\n",
        "\n",
        "5. **What is the primary abstraction in Spark?**\n",
        "   - **c) RDD**\n",
        "\n",
        "6. **What is a DataFrame in Spark?**\n",
        "   - **a) A distributed collection of data organized into named columns**"
      ],
      "metadata": {
        "id": "fftSoSWsDD8A"
      },
      "id": "fftSoSWsDD8A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ec0385",
      "metadata": {
        "id": "85ec0385"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5603ebed",
      "metadata": {
        "id": "5603ebed"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9b9e19e7",
      "metadata": {
        "id": "9b9e19e7"
      },
      "source": [
        "# Apache Spark - Classification Model Assignment\n",
        "\n",
        "## Question: You are required to generate a dataset using Apache Spark's StructType and StructField. The dataset should contain at least 1000 rows and have the following features:\n",
        "\n",
        "    age (Integer)\n",
        "    salary (Float)\n",
        "    experience (Integer)\n",
        "    education_level (String: [\"High School\", \"Bachelor\", \"Master\", \"PhD\"])\n",
        "    job_role (String: [\"Engineer\", \"Manager\", \"Analyst\", \"Clerk\"])\n",
        "    hired (Label - Integer: 1 if hired, 0 if not hired)\n",
        "\n",
        "\n",
        "\n",
        "    Tasks:\n",
        "    Generate a synthetic dataset using Apache Spark with the above schema.\n",
        "    Use VectorAssembler to convert features into a format suitable for MLlib.\n",
        "    Split the dataset into training (80%) and testing (20%) sets.\n",
        "    Train a classification model (Logistic Regression, Decision Tree, or Random Forest).\n",
        "    Evaluate the model using accuracy and F1-score.\n",
        "    Display the predictions on the test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d74dbd5c",
      "metadata": {
        "id": "d74dbd5c"
      },
      "source": [
        "## Example Code for Dataset Generation & Model Training"
      ]
    },
    {
      "cell_type": "raw",
      "id": "e5617dd9",
      "metadata": {
        "id": "e5617dd9"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import random\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"SparkClassification\").getOrCreate()\n",
        "\n",
        "# Define schema\n",
        "schema = StructType([\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"salary\", FloatType(), True),\n",
        "    StructField(\"experience\", IntegerType(), True),\n",
        "    StructField(\"education_level\", StringType(), True),\n",
        "    StructField(\"job_role\", StringType(), True),\n",
        "    StructField(\"hired\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Generate synthetic data\n",
        "data = []\n",
        "education_levels = [\"High School\", \"Bachelor\", \"Master\", \"PhD\"]\n",
        "job_roles = [\"Engineer\", \"Manager\", \"Analyst\", \"Clerk\"]\n",
        "\n",
        "for _ in range(1000):\n",
        "    age = random.randint(22, 60)\n",
        "    salary = round(random.uniform(30000, 120000), 2)\n",
        "    experience = random.randint(0, 30)\n",
        "    education_level = random.choice(education_levels)\n",
        "    job_role = random.choice(job_roles)\n",
        "    hired = 1 if (experience > 5 and salary > 50000) else 0\n",
        "    data.append((age, salary, experience, education_level, job_role, hired))\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Convert categorical columns to numerical\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=\"education_level\", outputCol=\"education_level_index\").fit(df),\n",
        "    StringIndexer(inputCol=\"job_role\", outputCol=\"job_role_index\").fit(df)\n",
        "]\n",
        "\n",
        "for indexer in indexers:\n",
        "    df = indexer.transform(df)\n",
        "\n",
        "# Assemble feature columns\n",
        "feature_cols = [\"age\", \"salary\", \"experience\", \"education_level_index\", \"job_role_index\"]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "df = assembler.transform(df).select(\"features\", \"hired\")\n",
        "\n",
        "# Split data\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train model\n",
        "lr = LogisticRegression(labelCol=\"hired\", featuresCol=\"features\")\n",
        "model = lr.fit(train_data)\n",
        "\n",
        "# Predictions\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"hired\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Model Accuracy: {accuracy}\")\n",
        "\n",
        "# Show predictions\n",
        "predictions.select(\"features\", \"hired\", \"prediction\").show(10)\n",
        "\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import random\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"SparkClassification\").getOrCreate()\n",
        "\n",
        "# Define schema\n",
        "schema = StructType([\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"salary\", FloatType(), True),\n",
        "    StructField(\"experience\", IntegerType(), True),\n",
        "    StructField(\"education_level\", StringType(), True),\n",
        "    StructField(\"job_role\", StringType(), True),\n",
        "    StructField(\"hired\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Generate synthetic data\n",
        "data = []\n",
        "education_levels = [\"High School\", \"Bachelor\", \"Master\", \"PhD\"]\n",
        "job_roles = [\"Engineer\", \"Manager\", \"Analyst\", \"Clerk\"]\n",
        "\n",
        "for _ in range(1000):\n",
        "    age = random.randint(22, 60)\n",
        "    salary = round(random.uniform(30000, 120000), 2)\n",
        "    experience = random.randint(0, 30)\n",
        "    education_level = random.choice(education_levels)\n",
        "    job_role = random.choice(job_roles)\n",
        "    hired = 1 if (experience > 5 and salary > 50000) else 0\n",
        "    data.append((age, salary, experience, education_level, job_role, hired))\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "# Convert categorical columns to numerical\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=\"education_level\", outputCol=\"education_level_index\").fit(df),\n",
        "    StringIndexer(inputCol=\"job_role\", outputCol=\"job_role_index\").fit(df)\n",
        "]\n",
        "\n",
        "for indexer in indexers:\n",
        "    df = indexer.transform(df)\n",
        "\n",
        "# Assemble feature columns\n",
        "feature_cols = [\"age\", \"salary\", \"experience\", \"education_level_index\", \"job_role_index\"]\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "df = assembler.transform(df).select(\"features\", \"hired\")\n",
        "\n",
        "# Split data\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Train Decision Tree model\n",
        "dt = DecisionTreeClassifier(labelCol=\"hired\", featuresCol=\"features\")\n",
        "model = dt.fit(train_data)\n",
        "\n",
        "# Predictions\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Evaluate model\n",
        "evaluator_accuracy = MulticlassClassificationEvaluator(labelCol=\"hired\", metricName=\"accuracy\")\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"hired\", metricName=\"f1\")\n",
        "\n",
        "accuracy = evaluator_accuracy.evaluate(predictions)\n",
        "f1_score = evaluator_f1.evaluate(predictions)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy}\")\n",
        "print(f\"Model F1 Score: {f1_score}\")\n",
        "\n",
        "# Show predictions\n",
        "predictions.select(\"features\", \"hired\", \"prediction\").show(10)\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Anl0d6DiAeso",
        "outputId": "7eada4de-2992-4a76-f0e9-a85e2692030b"
      },
      "id": "Anl0d6DiAeso",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.978021978021978\n",
            "Model F1 Score: 0.978111307470473\n",
            "+--------------------+-----+----------+\n",
            "|            features|hired|prediction|\n",
            "+--------------------+-----+----------+\n",
            "|[22.0,45895.03906...|    0|       0.0|\n",
            "|[22.0,63284.62109...|    1|       1.0|\n",
            "|[22.0,110749.2109...|    1|       1.0|\n",
            "|[23.0,76079.9375,...|    0|       0.0|\n",
            "|[24.0,32844.19921...|    0|       0.0|\n",
            "|[24.0,70626.00781...|    1|       1.0|\n",
            "|[24.0,102656.4218...|    1|       1.0|\n",
            "|[25.0,44254.73828...|    0|       0.0|\n",
            "|[26.0,39551.94140...|    0|       0.0|\n",
            "|[26.0,44443.57031...|    0|       0.0|\n",
            "+--------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}